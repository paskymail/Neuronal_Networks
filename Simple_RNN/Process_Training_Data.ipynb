{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Process training Data to train a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pprint as pprint\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.1.0\n"
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### 1. Get the data from CSV files\n",
    "First Import it using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_action (class_int):\n",
    "    with open('./data/class_to_action (28).json') as json_file:\n",
    "        CtA = json.load(json_file)\n",
    "        \n",
    "    action = CtA[class_int]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data (file):\n",
    "    raw_dataset = pd.read_csv(file)\n",
    "    dataset = raw_dataset.copy()\n",
    "    training_size = len(dataset)\n",
    "    N_CATEGORIES = 28\n",
    "\n",
    "    dataset[\"A0\"] = np.nan\n",
    "    dataset[\"A1\"] = np.nan\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        CS_array =np.array(eval(row[\"current_state\"]))\n",
    "        dataset[\"current_state\"][index] = CS_array\n",
    "        GS_array =np.array(eval(row[\"goal_state\"]))\n",
    "        dataset[\"goal_state\"][index] = GS_array\n",
    "        dataset[\"A0\"][index] = class_to_action(str(dataset[\"out_action\"][index]))[0]\n",
    "        dataset[\"A1\"][index] = class_to_action(str(dataset[\"out_action\"][index]))[1]\n",
    "\n",
    "    # Usar estado final \n",
    "    dataset[\"Difference\"] =   dataset[\"goal_state\"]-dataset[\"current_state\"]\n",
    "\n",
    "    dataset[[\"C0\",'C1',\"C2\",\"C3\",'C4',\"C5\"]] = pd.DataFrame(dataset.current_state.values.tolist(), index= dataset.index)\n",
    "    dataset[[\"D0\",'D1',\"D2\",\"D3\",'D4',\"D5\"]] = pd.DataFrame(dataset.Difference.values.tolist(), index= dataset.index)\n",
    "    \n",
    "    dataset_short= dataset[[\"id_camino\",\"D0\",'D1',\"D2\",\"D3\",'D4',\"D5\",\"out_action\",\"A0\",\"A1\"]]\n",
    "\n",
    "    return dataset_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "It is good practice to normalize features that use different scales and ranges. Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\n",
    "\n",
    "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, train_stats):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset_short,action_vector):\n",
    "    reduced_dataset = dataset_short.copy()\n",
    "    reduced_dataset.pop(\"out_action\")\n",
    "    reduced_dataset.pop(\"A0\")\n",
    "    reduced_dataset.pop(\"A1\")\n",
    "    reduced_dataset.pop(\"id_camino\")\n",
    "    data_stats = reduced_dataset.describe()\n",
    "    data_stats = data_stats.transpose()\n",
    "\n",
    "    data_stats.to_csv(r\"./data/data_stats.csv\")\n",
    "\n",
    "    action_vector.append(\"id_camino\")\n",
    "    normed_data = norm(reduced_dataset, data_stats)\n",
    "    normed_dataset = normed_data.join(dataset_short[action_vector])\n",
    "\n",
    "    return normed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_padding(normed_dataset,N_STEPS):\n",
    "    PAD_LEN = N_STEPS+1\n",
    "    padded_data = []\n",
    "    for camino in normed_dataset[\"id_camino\"]:\n",
    "        group = normed_dataset[normed_dataset[\"id_camino\"]==camino]\n",
    "        group.pop(\"id_camino\")\n",
    "        padded_data.append(group.values)\n",
    "    \n",
    "    dataset2 = tf.keras.preprocessing.sequence.pad_sequences(padded_data, maxlen=PAD_LEN, dtype='float64', padding='post', truncating='post', value=0.0)\n",
    "\n",
    "    return dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuiClDk45eS4"
   },
   "source": [
    "This normalized data is what we will use to train the model.\n",
    "\n",
    "Caution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model.  That includes the test set as well as live data when the model is used in production."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pasky': virtualenv)",
   "name": "python37564bitpaskyvirtualenvdb13efe5d4c0467fbbb744965ba97dd5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}