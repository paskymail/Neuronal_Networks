{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Process training Data to train a RNN"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "EIdT9iu_Z4Rb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
    "import pathlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "import pprint as pprint\r\n",
    "import math\r\n",
    "import time\r\n",
    "import array "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "try:\r\n",
    "  # %tensorflow_version only exists in Colab.\r\n",
    "  %tensorflow_version 2.x\r\n",
    "except Exception:\r\n",
    "  pass\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow_docs as tfdocs\r\n",
    "import tensorflow_docs.plots\r\n",
    "import tensorflow_docs.modeling\r\n",
    "import os\r\n",
    "import json\r\n",
    "import pickle\r\n",
    "import sklearn\r\n",
    "from sklearn import neighbors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Pre-process data"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "def get_data (file):\r\n",
    "    raw_dataset = pd.read_csv(file)\r\n",
    "    dataset = raw_dataset.copy()\r\n",
    "    training_size = len(dataset)\r\n",
    "    N_CATEGORIES = 28\r\n",
    "\r\n",
    "    dataset[\"A0\"] = np.nan\r\n",
    "    dataset[\"A1\"] = np.nan\r\n",
    "\r\n",
    "    for index, row in dataset.iterrows():\r\n",
    "        CS_array =np.array(eval(row[\"current_state\"]))\r\n",
    "        dataset[\"current_state\"][index] = CS_array\r\n",
    "        GS_array =np.array(eval(row[\"goal_state\"]))\r\n",
    "        dataset[\"goal_state\"][index] = GS_array\r\n",
    "        GS_array =np.array(eval(row[\"initial_state\"]))\r\n",
    "        dataset[\"initial_state\"][index] = GS_array\r\n",
    "        GS_array =np.array(eval(row[\"out_action\"]))\r\n",
    "        dataset[\"out_action\"][index] = GS_array\r\n",
    "\r\n",
    "    # Usar estado final \r\n",
    "    dataset[\"Difference\"] =   dataset[\"goal_state\"]-dataset[\"current_state\"]\r\n",
    "\r\n",
    "    dataset[[\"C0\",'C1',\"C2\",\"C3\",'C4',\"C5\"]] = pd.DataFrame(dataset.current_state.values.tolist(), index= dataset.index)\r\n",
    "    dataset[[\"D0\",'D1',\"D2\",\"D3\",'D4',\"D5\"]] = pd.DataFrame(dataset.Difference.values.tolist(), index= dataset.index)\r\n",
    "    dataset[[\"A0\",\"A1\"]] = pd.DataFrame(dataset.out_action.values.tolist(), index= dataset.index)\r\n",
    "\r\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def add_end_state(dataset_short):\r\n",
    "    extended_dataset = dataset_short.copy()\r\n",
    "    exit_state = pd.DataFrame([{\"id_camino\":0,\"D0\":0,'D1':0,\"D2\":0,\"D3\":0,'D4':0,\"D5\":0,\"A0\":-0,\"A1\":-0}])\r\n",
    "    for camino in extended_dataset[\"id_camino\"].unique():\r\n",
    "        exit_state[\"id_camino\"] = camino\r\n",
    "        extended_dataset = extended_dataset.append(exit_state,ignore_index=True)\r\n",
    "\r\n",
    "    return extended_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def extend_dataset(dataset_short, N_STEPS):\r\n",
    "    extended_dataset = dataset_short.copy()\r\n",
    "    last = extended_dataset.iloc[-1,0]\r\n",
    "    i=1\r\n",
    "    for camino in extended_dataset[\"id_camino\"].unique():\r\n",
    "        len_camino = len(extended_dataset[extended_dataset[\"id_camino\"]==camino])\r\n",
    "        if(len_camino>N_STEPS):\r\n",
    "            tail= extended_dataset[extended_dataset[\"id_camino\"]==camino][len_camino-N_STEPS:]\r\n",
    "            tail[\"id_camino\"]= last+i\r\n",
    "            i=i+1\r\n",
    "            extended_dataset = extended_dataset.append(tail,ignore_index=True)\r\n",
    "\r\n",
    "    return extended_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Action treatment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def closest_category(X, pred):\r\n",
    "    tree = sklearn.neighbors.KDTree(X, leaf_size=2)\r\n",
    "    dist, ind = tree.query(pred, k=1)\r\n",
    "    return ind"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def class_to_action (class_int):\r\n",
    "    with open('./data/class_to_action (28).json') as json_file:\r\n",
    "        CtA = json.load(json_file)\r\n",
    "        \r\n",
    "    action = CtA[class_int]\r\n",
    "    return action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalize the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def norm(x, train_stats):\r\n",
    "  #return (x - train_stats['mean']) / train_stats['std']\r\n",
    "  return (x) / train_stats['std']\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def denorm(x, train_stats):\r\n",
    "  #return (x - train_stats['mean']) / train_stats['std']\r\n",
    "  return (x) * train_stats['std']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def normalize_data(dataset_short,action_vector):\r\n",
    "    reduced_dataset = dataset_short.copy()\r\n",
    "    reduced_dataset.pop(\"A0\")\r\n",
    "    reduced_dataset.pop(\"A1\")\r\n",
    "    reduced_dataset.pop(\"id_camino\")\r\n",
    "    data_stats = reduced_dataset.describe()\r\n",
    "    data_stats = data_stats.transpose()\r\n",
    "\r\n",
    "    action_vector.append(\"id_camino\")\r\n",
    "    normed_data = norm(reduced_dataset, data_stats)\r\n",
    "    normed_dataset = normed_data.join(dataset_short[action_vector])\r\n",
    "\r\n",
    "    return normed_dataset, data_stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pad the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def padding(normed_dataset,N_STEPS, position):\r\n",
    "    PAD_LEN = N_STEPS+1\r\n",
    "    padded_data = []\r\n",
    "    for camino in normed_dataset[\"id_camino\"].unique():\r\n",
    "        group = normed_dataset[normed_dataset[\"id_camino\"]==camino]\r\n",
    "        group.pop(\"id_camino\")\r\n",
    "        padded_data.append(group.values)\r\n",
    "    \r\n",
    "    dataset2 = tf.keras.preprocessing.sequence.pad_sequences(padded_data, maxlen=PAD_LEN, dtype='float64', padding=position, truncating=position, value=0.0)\r\n",
    "\r\n",
    "    return dataset2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def group_by_path(normed_dataset):\r\n",
    "    dataset2 = []\r\n",
    "    for camino in normed_dataset[\"id_camino\"].unique():\r\n",
    "        group = normed_dataset[normed_dataset[\"id_camino\"]==camino]\r\n",
    "        group.pop(\"id_camino\")\r\n",
    "        dataset2.append(group.values)\r\n",
    "    \r\n",
    "    return np.array(dataset2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def multiplex_trajectories(dataset2):\r\n",
    "    multitrajectory = dataset2.tolist()\r\n",
    "\r\n",
    "    for trajectory in dataset2:\r\n",
    "        len_trajectory = len(trajectory)\r\n",
    "        for i in range(2,len_trajectory):\r\n",
    "            subtrajectory= trajectory[0:i]\r\n",
    "            # new_trajectory = np.expand_dims(subtrajectory, axis=0)\r\n",
    "            # array_tuple = (multitrajectory, new_trajectory)\r\n",
    "            # multitrajectory = np.vstack(  array_tuple )\r\n",
    "            multitrajectory.append(subtrajectory)\r\n",
    "\r\n",
    "    return np.asarray(multitrajectory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "file = \"OSPA_training_data_short.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "STEPS = 50\r\n",
    "\r\n",
    "dataset = get_data(\"./data/\"+file)\r\n",
    "\r\n",
    "dataset_distances= dataset[[\"id_camino\",\"D0\",'D1',\"D2\",\"D3\",'D4',\"D5\",\"A0\",\"A1\"]]\r\n",
    "\r\n",
    "exteneded_dataset= add_end_state(dataset_distances)\r\n",
    "\r\n",
    "action_vector = []\r\n",
    "normed_dataset, data_stats = normalize_data(exteneded_dataset, action_vector)\r\n",
    "\r\n",
    "dataset2 = group_by_path(normed_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\pasky\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\pasky\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\pasky\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\pasky\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from ast import literal_eval\r\n",
    "def print_model_df_analysis(csv_name =\"./data/model_data_csv.csv\", percent_value=25/100.0):\r\n",
    "    df = pd.read_csv(csv_name, converters={\"target\":literal_eval})\r\n",
    "    df[\"abs_x\"] = df[\"target\"].apply(lambda x: abs(x[0]))\r\n",
    "    df[\"abs_z\"] = df[\"target\"].apply(lambda x: abs(x[1]))\r\n",
    "\r\n",
    "    return df.loc[df[\"abs_x\"]*percent_value>df[\"abs_z\"]]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def find_zvalue(x, sample_path, from_index, x_index=0, z_index=1):\r\n",
    "    while x>sample_path[from_index][x_index]:\r\n",
    "        from_index += 1\r\n",
    "    prev_z = sample_path[from_index - 1][z_index]\r\n",
    "    next_z = sample_path[from_index][z_index]\r\n",
    "\r\n",
    "    prev_x = sample_path[from_index -1][x_index]\r\n",
    "    next_x = sample_path[from_index][x_index]\r\n",
    "\r\n",
    "    z_value = (next_z - prev_z) / (next_x-prev_x)*(x-prev_x)+prev_z\r\n",
    "    return z_value, from_index\r\n",
    "\r\n",
    "def get_mean_error(ospa_path, model_path, num=10, eps = 5):\r\n",
    "    s0= ospa_path[0]\r\n",
    "    sf = ospa_path[-1]\r\n",
    "    x_values = np.linspace(s0[0]+eps, sf[0]-eps, num = num+1)\r\n",
    "\r\n",
    "    min_ospa_index, min_model_index = 0, 0\r\n",
    "    error=0\r\n",
    "    for x in x_values:\r\n",
    "        ospa_z, min_ospa_index = find_zvalue(x,ospa_path, min_ospa_index)\r\n",
    "        try:\r\n",
    "            model_z, min_model_index = find_zvalue(x, model_path, min_model_index)\r\n",
    "            error += abs(model_z-ospa_z)\r\n",
    "        except Exception as e:\r\n",
    "            print(e)\r\n",
    "    return error/(num+1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "interpreter": {
   "hash": "023a8dd934ca51c8a8ef6fdb3c1e7b5852f5a8916b902721ea367a3b1550ba39"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}