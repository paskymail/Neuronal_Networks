{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Process training Data to train a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pprint as pprint\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### 1. Get the data from CSV files\n",
    "First Import it using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_action (class_int):\n",
    "    with open('./data/class_to_action (28).json') as json_file:\n",
    "        CtA = json.load(json_file)\n",
    "        \n",
    "    action = CtA[class_int]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_category(X, pred):\n",
    "    tree = sklearn.neighbors.KDTree(X, leaf_size=2)\n",
    "    dist, ind = tree.query(pred, k=1)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data (file):\n",
    "    raw_dataset = pd.read_csv(file)\n",
    "    dataset = raw_dataset.copy()\n",
    "    training_size = len(dataset)\n",
    "    N_CATEGORIES = 28\n",
    "\n",
    "    dataset[\"A0\"] = np.nan\n",
    "    dataset[\"A1\"] = np.nan\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        CS_array =np.array(eval(row[\"current_state\"]))\n",
    "        dataset[\"current_state\"][index] = CS_array\n",
    "        GS_array =np.array(eval(row[\"goal_state\"]))\n",
    "        dataset[\"goal_state\"][index] = GS_array\n",
    "        GS_array =np.array(eval(row[\"initial_state\"]))\n",
    "        dataset[\"initial_state\"][index] = GS_array\n",
    "        GS_array =np.array(eval(row[\"out_action\"]))\n",
    "        dataset[\"out_action\"][index] = GS_array\n",
    "\n",
    "    # Usar estado final \n",
    "    dataset[\"Difference\"] =   dataset[\"goal_state\"]-dataset[\"current_state\"]\n",
    "\n",
    "    dataset[[\"C0\",'C1',\"C2\",\"C3\",'C4',\"C5\"]] = pd.DataFrame(dataset.current_state.values.tolist(), index= dataset.index)\n",
    "    dataset[[\"D0\",'D1',\"D2\",\"D3\",'D4',\"D5\"]] = pd.DataFrame(dataset.Difference.values.tolist(), index= dataset.index)\n",
    "    dataset[[\"A0\",\"A1\"]] = pd.DataFrame(dataset.out_action.values.tolist(), index= dataset.index)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the last state with the exit action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_state(dataset_short):\n",
    "    extended_dataset = dataset_short.copy()\n",
    "    exit_state = pd.DataFrame([{\"id_camino\":0,\"D0\":0,'D1':0,\"D2\":0,\"D3\":0,'D4':0,\"D5\":0,\"A0\":-0,\"A1\":-0}])\n",
    "    for camino in extended_dataset[\"id_camino\"].unique():\n",
    "        exit_state[\"id_camino\"] = camino\n",
    "        extended_dataset = extended_dataset.append(exit_state,ignore_index=True)\n",
    "\n",
    "    return extended_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_dataset(dataset_short, N_STEPS):\n",
    "    extended_dataset = dataset_short.copy()\n",
    "    last = extended_dataset.iloc[-1,0]\n",
    "    i=1\n",
    "    for camino in extended_dataset[\"id_camino\"].unique():\n",
    "        len_camino = len(extended_dataset[extended_dataset[\"id_camino\"]==camino])\n",
    "        if(len_camino>N_STEPS):\n",
    "            tail= extended_dataset[extended_dataset[\"id_camino\"]==camino][len_camino-N_STEPS:]\n",
    "            tail[\"id_camino\"]= last+i\n",
    "            i=i+1\n",
    "            extended_dataset = extended_dataset.append(tail,ignore_index=True)\n",
    "\n",
    "    return extended_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "It is good practice to normalize features that use different scales and ranges. Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\n",
    "\n",
    "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, train_stats):\n",
    "  #return (x - train_stats['mean']) / train_stats['std']\n",
    "  return (x) / train_stats['std']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x, train_stats):\n",
    "  #return (x - train_stats['mean']) / train_stats['std']\n",
    "  return (x) * train_stats['std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(dataset_short,action_vector):\n",
    "    reduced_dataset = dataset_short.copy()\n",
    "    reduced_dataset.pop(\"A0\")\n",
    "    reduced_dataset.pop(\"A1\")\n",
    "    reduced_dataset.pop(\"id_camino\")\n",
    "    data_stats = reduced_dataset.describe()\n",
    "    data_stats = data_stats.transpose()\n",
    "\n",
    "    action_vector.append(\"id_camino\")\n",
    "    normed_data = norm(reduced_dataset, data_stats)\n",
    "    normed_dataset = normed_data.join(dataset_short[action_vector])\n",
    "\n",
    "    return normed_dataset, data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(normed_dataset,N_STEPS, position):\n",
    "    PAD_LEN = N_STEPS+1\n",
    "    padded_data = []\n",
    "    for camino in normed_dataset[\"id_camino\"].unique():\n",
    "        group = normed_dataset[normed_dataset[\"id_camino\"]==camino]\n",
    "        group.pop(\"id_camino\")\n",
    "        padded_data.append(group.values)\n",
    "    \n",
    "    dataset2 = tf.keras.preprocessing.sequence.pad_sequences(padded_data, maxlen=PAD_LEN, dtype='float64', padding=position, truncating=position, value=0.0)\n",
    "\n",
    "    return dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_path(normed_dataset):\n",
    "    dataset2 = []\n",
    "    for camino in normed_dataset[\"id_camino\"].unique():\n",
    "        group = normed_dataset[normed_dataset[\"id_camino\"]==camino]\n",
    "        group.pop(\"id_camino\")\n",
    "        dataset2.append(group.values)\n",
    "    \n",
    "    return np.array(dataset2)"
   ]
  },
  {
   "source": [
    "# Evaluate the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def print_model_df_analysis(csv_name =\"./data/model_data_csv.csv\", percent_value=25/100.0):\n",
    "    df = pd.read_csv(csv_name, converters={\"target\":literal_eval})\n",
    "    df[\"abs_x\"] = df[\"target\"].apply(lambda x: abs(x[0]))\n",
    "    df[\"abs_z\"] = df[\"target\"].apply(lambda x: abs(x[1]))\n",
    "\n",
    "    return df.loc[df[\"abs_x\"]*percent_value>df[\"abs_z\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zvalue(x, sample_path, from_index, x_index=0, z_index=1):\n",
    "    while x>sample_path[from_index][x_index]:\n",
    "        from_index += 1\n",
    "    prev_z = sample_path[from_index - 1][z_index]\n",
    "    next_z = sample_path[from_index][z_index]\n",
    "\n",
    "    prev_x = sample_path[from_index -1][x_index]\n",
    "    next_x = sample_path[from_index][x_index]\n",
    "\n",
    "    z_value = (next_z - prev_z) / (next_x-prev_x)*(x-prev_x)+prev_z\n",
    "    return z_value, from_index\n",
    "\n",
    "def get_mean_error(ospa_path, model_path, num=10, eps = 5):\n",
    "    s0= ospa_path[0]\n",
    "    sf = ospa_path[-1]\n",
    "    x_values = np.linspace(s0[0]+eps, sf[0]-eps, num = num+1)\n",
    "\n",
    "    min_ospa_index, min_model_index = 0, 0\n",
    "    error=0\n",
    "    for x in x_values:\n",
    "        ospa_z, min_ospa_index = find_zvalue(x,ospa_path, min_ospa_index)\n",
    "        try:\n",
    "            model_z, min_model_index = find_zvalue(x, model_path, min_model_index)\n",
    "            error += abs(model_z-ospa_z)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return error/(num+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"OSPA_landing_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= get_data (\"./data/\"+file)\n",
    "dataset_distances= dataset[[\"id_camino\",\"D0\",'D1',\"D2\",\"D3\",'D4',\"D5\",\"A0\",\"A1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trajectories = dataset[[\"id_camino\",\"C0\",'C1',\"C2\",\"C3\",'C4',\"C5\",\"A0\",\"A1\",\"initial_state\", \"goal_state\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pasky': virtualenv)",
   "name": "python37564bitpaskyvirtualenvdb13efe5d4c0467fbbb744965ba97dd5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}